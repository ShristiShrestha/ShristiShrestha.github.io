[
    {
        "title": "Automatic Summarization of Mobile Application Reviews using Chain of Density Prompting",
        "url": "https://arxiv.org/pdf/2506.14192",
        "authors": ["S. Shrestha", " and A. Mahmoud, "],
        "venue": "Automated Software Engineering Journal",
        "venue_bold": "(ASEJ)",
        "year": "2025",
        "problem": "Popular apps receive thousands of reviews daily. Users cannot read them all to make informed decisions, and existing star-rating systems fail to capture specific user needs. For example, stock trading app users expect seamless transaction executions, transparent pricing, no hidden fees, and strategy resources for maximizing their returns on investments (ROI). Users navigating current app review systems are forced to navigate a diverse array of feedback, including bug reports and feature requests, to determine if the app matches their user goals.",
        "solution": "We propose an LLM-based app review summarization technique to extract and present fixed length summaries for apps. We employ the Chain of Density prompting, originally designed for news articles. From a large volume of reviews, the LLM is instructed to identify review entities and fuse them into a fixed-length summary. Unlike news articles, app reviews represent voices of thousands of users and discuss various topics related to the app.",
        "data": "Summarized a dataset of 37,245 reviews collected from eight popular apps across four diverse domains: Ride-hailing, Online Dating, Investing, and Mental Health. For fair representation of different ratings in the reviews, we employed stratified sampling based on star ratings (1-5 stars) and used Hybrid TF-IDF ranking to sample 2.8k high-information reviews. We used GPT-4 to generate abstractive summaries of the sampled reviews.",
        "results": "The modified version of CoD prompting achieved an average recall of 81% for key review entities, significantly outperforming vanilla prompts (64%) and extractive summarization (60%) baselines. Our user study with 48 participants demonstrated that the generated summaries maintained high readability even as their semantic density increased. Comparative analysis revealed that while GPT-4 produced the most balanced summaries in terms of coherence and density, Gemini-1.5-Flash and Llama-3.1 offered lower-cost alternatives with varying trade-offs in summary length and sentiment retention.",
        "learnt": ["LLM-based text summarization", "Recall analysis", "Performance analysis of multiple LLMs", "Cost-latency analysis of LLMs", "Review entity analysis", "Readability user study", "Non-parametric statistical analysis"],
        "github": "https://github.com/ShristiShrestha/Summarize-Mobile-App-Reviews",
        "prototype": "https://github.com/ShristiShrestha/Summarize-Mobile-App-Reviews",
        "figma": "https://github.com/ShristiShrestha/Summarize-Mobile-App-Reviews"
    },
    {
        "title": "No Country for Indie Developers: A Study of Google Play's Closed Testing Requirements for New Personal Developer Accounts",
        "url": "https://dl.acm.org/doi/pdf/10.1145/3736578",
        "authors": ["G. Shrestha, ", "S. Shrestha,", " and A. Mahmoud, "],
        "venue": "ACM Transactions on Software Engineering and Methodology",
        "venue_bold": "(ACM TOSEM)",
        "year": "2025",
        "problem": "Put yourself in the position of an indie developer. You are aiming to solve a niche problem, spent months in building a viable product, and want to release it to start attracting real users. Google's Play Store wants you to find 20 individuals willing to test (interact) your app for 14 days continuously. As per the documentation, you can reach out to your friends, family, colleagues, or social media forums to find those individuals.",
        "solution": "We conducted a mixed-method study to uncover how developers are surviving the policy. First, we performed qualitative thematic analysis of Reddit comments about the policy. Second, we ran a 15-min survey with developers who faced the policy in releasing their own apps on Play Store.",
        "data": "We analyzed 897 comments from 564 users posted on 38 subreddit threads. Then, we surveyed 14 indie developers, asking them two questions about the policy: a) strategies to comply with the policy, b) assessment of the policy requirements.",
        "results": "We found developers expressing frustration on several Reddit posts and shared their logistical barriers to adopting the policy. Several job posts appeared on online Gig platforms like Fiverr asking/calling for testers. Redditors reported concerns over such growing a market (e.g., `potential scams`, `black market`, `sweatshops`). Occasionally, developers expressed that the policy discriminated them in favor of corporate developers.",
        "learnt": ["Qualitative thematic analysis",  "Developer survey", "Inter-rater reliability", "Play Store App Testing Policy"],
        "github": "https://github.com/ShristiShrestha/Google-App-Testing-Policy-Analysis",
        "prototype": "",
        "figma": ""         
    },
    {
        "title": "Generating Rate Features for Mobile Applications",
        "url": "https://dl.acm.org/doi/pdf/10.1145/3647632.3647986",
        "authors": ["S. Shrestha", " and A. Mahmoud, "],
        "venue": "Proceedings of the IEEE/ACM International Conference on Mobile Software Engineering and Systems",
        "venue_bold": "(ICSE MobileSoft)",
        "year": "2024",
        "problem": "Finding an app that meets specific user criteria is difficult because the current `one-size-fits-all` 5-star rating system in app stores fails to capture domain-specific needs. This forces users to spend significant mental effort scrolling through thousands of unstructured reviews to locate relevant information.",
        "solution": "We developed an automated, unsupervised pipeline that uses a large language model (LLM) to generate `Rate Features` from long, unstructured app reviews. `Rate features` are neutral, domain-specific, concise summaries  (2-3 words) of the reviews. We first filter informative reviews using Hybrid TF.IDF extractive summarization method. Then, we use GPT-3.5-turbo to generate `Rate Features` from the filtered reviews.",
        "data": "Generated `Rate Features` from 167k reviews of 90 popular apps across three domains: Ride-hailing, Mental Health, and Investing. ",
        "results": "We found that LLMs identified user goals described in the reviews as the best candidates for Rate features. Top three frequently appearing `Rate Features` recalled 95-100% of the user goals in the analyzed review dataset.",
        "learnt": ["Prompt engineering and analysis", "Zero-shot prompting", "NLTK", "Gensim", "GloVe embeddings", "Qualitative thematic analysis", "Recall analysis", "Pandas", "Scikit-learn"],
        "github": "https://github.com/ShristiShrestha/Generate-App-Rate-Features",
        "prototype": "https://github.com/ShristiShrestha/Generate-App-Rate-Features",
        "figma": "https://github.com/ShristiShrestha/Generate-App-Rate-Features"
    }
]
